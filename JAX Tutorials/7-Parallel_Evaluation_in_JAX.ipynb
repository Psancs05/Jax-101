{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab TPU Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re running this code in Google Colab, be sure to choose TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tools.colab_tpu\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import NamedTuple, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)\n",
    "w = np.array([2., 3., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(x, w):\n",
    "  output = []\n",
    "  for i in range(1, len(x)-1):\n",
    "    output.append(jnp.dot(x[i-1:i+2], w))\n",
    "  return jnp.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolve(x,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s convert our convolve function into one that runs on entire batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_devices = jax.local_device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(5 * n_devices).reshape(-1, 5)\n",
    "ws = np.stack([w] * n_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(convolve)(xs, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To spread out the computation across multiple devices, just replace jax.vmap with jax.pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(convolve)(xs,ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(convolve)(xs, jax.pmap(convolve)(xs,ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying in_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(convolve, in_axes=(0, None))(xs, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pmap and jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jax.pmap JIT-compiles the function given to it as part of its operation, so there is no need to additionally jax.jit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication between devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_convolution(x, w):\n",
    "  output = []\n",
    "  for i in range(1, len(x)-1):\n",
    "    output.append(jnp.dot(x[i-1:i+2], w))\n",
    "  output = jnp.array(output)\n",
    "  return output / jax.lax.psum(output, axis_name='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(normalized_convolution, axis_name='p')(xs, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(normalized_convolution, axis_name='p')(xs, ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting jax.pmap and jax.vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "  weight: jnp.ndarray\n",
    "  bias: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(rng) -> Params:\n",
    "  \"\"\"Returns the initial model params.\"\"\"\n",
    "  weights_key, bias_key = jax.random.split(rng)\n",
    "  weight = jax.random.normal(weights_key, ())\n",
    "  bias = jax.random.normal(bias_key, ())\n",
    "  return Params(weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n",
    "  pred = params.weight * xs + params.bias\n",
    "  return jnp.mean((pred - ys) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far, the code is identical to the single-device case. Here's what's new:\n",
    "\n",
    "# Remember that the `axis_name` is just an arbitrary string label used\n",
    "# to later tell `jax.lax.pmean` which axis to reduce over. Here, we call it\n",
    "# 'num_devices', but could have used anything, so long as `pmean` used the same.\n",
    "@functools.partial(jax.pmap, axis_name='num_devices')\n",
    "def update(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> Tuple[Params, jnp.ndarray]:\n",
    "  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n",
    "\n",
    "  # Compute the gradients on the given minibatch (individually on each device).\n",
    "  loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)\n",
    "\n",
    "  # Combine the gradient across all devices (by taking their mean).\n",
    "  grads = jax.lax.pmean(grads, axis_name='num_devices')\n",
    "\n",
    "  # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
    "  loss = jax.lax.pmean(loss, axis_name='num_devices')\n",
    "\n",
    "  # Each device performs its own update, but since we start with the same params\n",
    "  # and synchronise gradients, the params stay in sync.\n",
    "  new_params = jax.tree_map(\n",
    "      lambda param, g: param - g * LEARNING_RATE, params, grads)\n",
    "\n",
    "  return new_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate true data from y = w*x + b + noise\n",
    "true_w, true_b = 2, -1\n",
    "xs = np.random.normal(size=(128, 1))\n",
    "noise = 0.5 * np.random.normal(size=(128, 1))\n",
    "ys = xs * true_w + true_b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise parameters and replicate across devices.\n",
    "params = init(jax.random.PRNGKey(123))\n",
    "n_devices = jax.local_device_count()\n",
    "replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(replicated_params.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr):\n",
    "  \"\"\"Splits the first axis of `arr` evenly across the number of devices.\"\"\"\n",
    "  return arr.reshape(n_devices, arr.shape[0] // n_devices, *arr.shape[1:])\n",
    "\n",
    "# Reshape xs and ys for the pmapped `update()`.\n",
    "x_split = split(xs)\n",
    "y_split = split(ys)\n",
    "\n",
    "print(type(x_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_after_update(name, obj):\n",
    "  print(f\"after first `update()`, `{name}` is a\", type(obj))\n",
    "\n",
    "# Actual training loop.\n",
    "for i in range(1000):\n",
    "\n",
    "  # This is where the params and data gets communicated to devices:\n",
    "  replicated_params, loss = update(replicated_params, x_split, y_split)\n",
    "\n",
    "  # The returned `replicated_params` and `loss` are now both ShardedDeviceArrays,\n",
    "  # indicating that they're on the devices.\n",
    "  # `x_split`, of course, remains a NumPy array on the host.\n",
    "  if i == 0:\n",
    "    type_after_update('replicated_params.weight', replicated_params.weight)\n",
    "    type_after_update('loss', loss)\n",
    "    type_after_update('x_split', x_split)\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    # Note that loss is actually an array of shape [num_devices], with identical\n",
    "    # entries, because each device returns its copy of the loss.\n",
    "    # So, we take the first element to print it.\n",
    "    print(f\"Step {i:3d}, loss: {loss[0]:.3f}\")\n",
    "\n",
    "\n",
    "# Plot results.\n",
    "\n",
    "# Like the loss, the leaves of params have an extra leading dimension,\n",
    "# so we take the params from the first device.\n",
    "params = jax.device_get(jax.tree_map(lambda x: x[0], replicated_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: hosts and devices in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "jax.devices()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMRMzIwPVTA3wz4egDHcMFO",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
